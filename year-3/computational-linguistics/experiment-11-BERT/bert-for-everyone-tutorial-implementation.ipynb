{"cells":[{"cell_type":"markdown","metadata":{},"source":["<center><h1 style=\"font-size:300%; font-family:cursive; color:navy;\">Why do we need BERT ?</h1></center>"]},{"cell_type":"markdown","metadata":{},"source":["<ul>\n","    <li style=\"font-size:150%; font-family:verdana;\">Proper language representation is key for general-purpose language understanding by machines. Context-free models such as <b>word2vec or GloVe</b> generate a single word embedding representation for each word in the vocabulary. For example, the word “bank” would have the same representation in “bank deposit” and in “riverbank”. Contextual models instead generate a representation of each word that is based on the other words in the sentence. <b>BERT</b>, as a contextual model, captures these relationships in a bidirectional way.</li>\n","    <br>\n","    <li style=\"font-size:150%; font-family:verdana;\">BERT was built upon recent work and clever ideas in pre-training contextual representations including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, the OpenAI Transformer, ULMFit and the Transformer. Although these models are all unidirectional or shallowly bidirectional, BERT is fully bidirectional.</li>\n","    <br>\n","    <li style=\"font-size:150%; font-family:verdana;\">The best part about <b>BERT</b> is that it can be download and used for free —  we can either use the  BERT models to extract high quality language features from our text data, or we can fine-tune these models on a specific task, like sentiment analysis and question answering, with our own data to produce state-of-the-art predictions</li>\n","</ul>"]},{"cell_type":"markdown","metadata":{},"source":["<center><h1 style=\"font-size:300%; font-family:cursive; color:navy;\">What is the core idea behind it?</h1></center>"]},{"cell_type":"markdown","metadata":{},"source":["<p style=\"font-size:150%; font-family:verdana;\">What is language modeling really about? Which problem are language models trying to solve? Basically, their task is to “fill in the blank” based on context. For example, given</p>\n","<br>\n","<center><h1 style=\"font-size:150%; font-family:cursive;\">“The woman went to the store and bought a _____ of shoes.”</h1></center>\n","<br>\n","<br>\n","<p style=\"font-size:150%; font-family:verdana;\">a language model(One-Directional Approach) might complete this sentence by saying that the word “cart” would fill the blank 20% of the time and the word “pair” 80% of the time.</p>\n","\n","<p style=\"font-size:150%; font-family:verdana;\"><b>Now enters BERT</b>, a language model which is bidirectionally trained (this is also its key technical innovation). This means we can now have a deeper sense of language context and flow compared to the single-direction language models. Unlike the previous language models, it takes both the previous and next tokens into account at the same time.</p>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">Moreover, BERT is based on the Transformer model architecture, instead of LSTMs.</p>"]},{"cell_type":"markdown","metadata":{},"source":["<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">BERT's ARCHITECTURE</h1></center>"]},{"cell_type":"markdown","metadata":{},"source":["<p style=\"font-size:150%; font-family:verdana;\">There are four types of pre-trained versions of BERT depending on the scale of the model architecture:</p>\n","<ol>\n","    <li style=\"font-size:150%; font-family:verdana;\">BERT-Base (Cased / Un-Cased): 12-layer, 768-hidden-nodes, 12-attention-heads, 110M parameters</li>\n","    <li style=\"font-size:150%; font-family:verdana;\">BERT-Large (Cased / Un-Cased): 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters</li>\n","</ol>\n","\n","<center><img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/09/bert_encoder.png\"></center>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">We need to choose <b>which BERT pre-trained weights we want</b>. For example, if we don’t have access to a Google TPU, we’d rather stick with the Base models. And then the choice of “cased” vs “uncased” depends on whether we think letter casing will be helpful for the task at hand.</p>"]},{"cell_type":"markdown","metadata":{},"source":["<center><h1 style=\"font-size:300%; font-family:cursive; color:navy;\">How does it work? - Let's Dive into it</h1></center>"]},{"cell_type":"markdown","metadata":{},"source":["<center><h1 style=\"font-size:200%; font-family:cursive;\">1. <u>Text - Preprocessing</u></h1></center>\n","<br>\n","<p style=\"font-size:150%; font-family:verdana;\">BERT relies on a Transformer (the attention mechanism that learns contextual relationships between words in a text). A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task. Since BERT’s goal is to generate a language representation model, it only needs the encoder part. The input to the encoder for BERT is a sequence of tokens, which are first converted into vectors and then processed in the neural network. For starters, every input embedding is a combination of 3 embeddings:</p>\n","\n","<center><img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/09/bert_emnedding.png\"></center>\n","<center><h1 style=\"font-size:150%; font-family:verdana;\">The input representation for BERT: The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.</h1></center>\n","<br>\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","<ul>\n","    <li style=\"font-size:150%; font-family:verdana;\"><b>Token embeddings:</b> A [CLS] token is added to the input word tokens at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.</li>\n","    <li style=\"font-size:150%; font-family:verdana;\"><b>Segment embeddings:</b>A marker indicating Sentence A or Sentence B is added to each token. This allows the encoder to distinguish between sentences.</li>\n","    <li style=\"font-size:150%; font-family:verdana;\"><b>Positional embeddings:</b>A positional embedding is added to each token to indicate its position in the sentence.</li>\n","</ul>\n","<br>\n","<center><h1 style=\"font-size:200%; font-family:cursive;\">2. <u>Pre-training Tasks</u></h1></center>\n","<br>\n","<p style=\"font-size:190%; font-family:verdana;\">BERT is pre-trained on two NLP tasks:</p>\n","\n","<h1 style=\"font-size:170%; font-family:cursive; color:navy;\"> 1. Masked Language Modelling</h1>\n","<ul>\n","    <li style=\"font-size:150%; font-family:verdana;\">Language Modeling is the task of predicting the next word given a sequence of words. In masked language modeling instead of predicting every next token, a percentage of input tokens is masked at random and only those masked tokens are predicted.</li>\n","    <li style=\"font-size:150%; font-family:verdana;\"> [MASK] Token - This is a token to denote that the token is missing</li>\n","    <li style=\"font-size:150%; font-family:verdana;\"> The masked words are not always replaced with the masked token – [MASK] because then the masked tokens would never be seen before fine-tuning. Therefore, 15% of the tokens are chosen at random. And out of the 15% of the tokens selected for masking:</li>\n","    <br>\n","    <center><h1 style=\"font-size:150%; font-family:cursive;\">80% of the tokens are actually replaced with the token [MASK].</h1></center>\n","    <center><h1 style=\"font-size:150%; font-family:cursive;\">10% of the time tokens are replaced with a random token.</h1></center>\n","    <center><h1 style=\"font-size:150%; font-family:cursive;\">10% of the time tokens are left unchanged.</h1></center>\n","</ul>\n","<br>\n","<h1 style=\"font-size:170%; font-family:cursive; color:navy;\"> 2. Next Sentence Prediction</h1>\n","<ul>\n","    <li style=\"font-size:150%; font-family:verdana;\">Next sentence prediction task is a binary classification task in which, given a pair of sentences, it is predicted if the second sentence is the actual next sentence of the first sentence.</li>\n","    <br>\n","    <center><img src=\"https://yashuseth.files.wordpress.com/2019/06/fig5.png?w=442&h=231\"></center>\n","    <br>\n","    <li style=\"font-size:150%; font-family:verdana;\">This task can be easily generated from any monolingual corpus. It is helpful because many downstream tasks such as Question and Answering and Natural Language Inference require an understanding of the relationship between two sentences.</li>\n","</ul>"]},{"cell_type":"markdown","metadata":{},"source":["<center><h1 style=\"font-size:200%; font-family:cursive; color:navy; height:65px;\">The model is trained with both Masked LM and Next Sentence Prediction together. This is to minimize the combined loss function of the two strategies — “together is better”.</h1></center>\n"]},{"cell_type":"markdown","metadata":{},"source":["<br>\n","<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">IMPLEMENTATION OF BERT</h1></center>"]},{"cell_type":"markdown","metadata":{},"source":["<p style=\"font-size:150%; font-family:verdana;\"><b>Problem Statement:</b> We have a collection of SMS messages. Some of these messages are spam and the rest are genuine. Our task is to build a system that would automatically detect whether a message is spam or not.</p>"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">1. Import Required Libraries & Dataset</h1>"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:43.768047Z","iopub.status.busy":"2021-05-20T12:16:43.767700Z","iopub.status.idle":"2021-05-20T12:16:43.773166Z","shell.execute_reply":"2021-05-20T12:16:43.772221Z","shell.execute_reply.started":"2021-05-20T12:16:43.767999Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-04-11 13:16:01.951929: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-11 13:16:04.386948: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2023-04-11 13:16:04.387230: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2023-04-11 13:16:04.387269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","\n","# specify GPU\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:45.214458Z","iopub.status.busy":"2021-05-20T12:16:45.214126Z","iopub.status.idle":"2021-05-20T12:16:45.244325Z","shell.execute_reply":"2021-05-20T12:16:45.243406Z","shell.execute_reply.started":"2021-05-20T12:16:45.214430Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>Ok lar... Joking wif u oni...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                               text\n","0      0  Go until jurong point, crazy.. Available only ...\n","1      0                      Ok lar... Joking wif u oni...\n","2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n","3      0  U dun say so early hor... U c already then say...\n","4      0  Nah I don't think he goes to usf, he lives aro..."]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(r\"./assets/data/spam-data.csv\")\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["<ul>\n","    <li style=\"font-size:150%;\">The dataset consists of two columns – “label” and “text”. The column “text” contains the message body and the “label” is a binary variable where 1 means spam and 0 means the message is not a spam.</li>\n","</ul>"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:45.963084Z","iopub.status.busy":"2021-05-20T12:16:45.962743Z","iopub.status.idle":"2021-05-20T12:16:45.972824Z","shell.execute_reply":"2021-05-20T12:16:45.971725Z","shell.execute_reply.started":"2021-05-20T12:16:45.963045Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0    0.865937\n","1    0.134063\n","Name: label, dtype: float64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# check class distribution\n","df['label'].value_counts(normalize = True)"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">2. Split the Dataset into train / test</h1>"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:46.733295Z","iopub.status.busy":"2021-05-20T12:16:46.732938Z","iopub.status.idle":"2021-05-20T12:16:46.749012Z","shell.execute_reply":"2021-05-20T12:16:46.748078Z","shell.execute_reply.started":"2021-05-20T12:16:46.733265Z"},"trusted":true},"outputs":[],"source":["# split train dataset into train, validation and test sets\n","train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n","                                                                    random_state=2018, \n","                                                                    test_size=0.3, \n","                                                                    stratify=df['label'])\n","\n","\n","val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n","                                                                random_state=2018, \n","                                                                test_size=0.5, \n","                                                                stratify=temp_labels)"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">3. Import Bert - base- uncased</h1>"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T11:49:24.187240Z","iopub.status.busy":"2021-05-20T11:49:24.186902Z","iopub.status.idle":"2021-05-20T11:50:15.137513Z","shell.execute_reply":"2021-05-20T11:50:15.135994Z","shell.execute_reply.started":"2021-05-20T11:49:24.187212Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e54bf95665c4c859d6de23c18a057ab","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20624a14b8e9456dafca363c1244a5bf","version_major":2,"version_minor":0},"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"ConnectionError","evalue":"HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py:438\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    441\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n","File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py:519\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    518\u001b[0m cache_content \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    520\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    521\u001b[0m     amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data\n\u001b[1;32m    522\u001b[0m ):  \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    529\u001b[0m     \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n","File \u001b[0;32m/usr/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n","File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n","File \u001b[0;32m/usr/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m/usr/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m \u001b[39melse\u001b[39;00m:\n","\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)","File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n","File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py:576\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 576\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    578\u001b[0m     \u001b[39mif\u001b[39;00m data:\n","File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py:512\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    510\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 512\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m    513\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m         \u001b[39m# cStringIO doesn't like amt=None\u001b[39;00m\n","File \u001b[0;32m/usr/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n","File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py:443\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    441\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeoutError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool, \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mRead timed out.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m BaseSSLError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m     \u001b[39m# FIXME: Is there a better way to differentiate between SSLErrors?\u001b[39;00m\n","\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# import BERT-base pretrained model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bert \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mbert-base-uncased\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Load the BERT tokenizer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:464\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    463\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    465\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m )\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2208\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2194\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m   2196\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[1;32m   2197\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2206\u001b[0m         _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[1;32m   2207\u001b[0m     )\n\u001b[0;32m-> 2208\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs)\n\u001b[1;32m   2210\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   2212\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m SAFE_WEIGHTS_NAME:\n\u001b[1;32m   2213\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1283\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m   1281\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1283\u001b[0m     http_get(\n\u001b[1;32m   1284\u001b[0m         url_to_download,\n\u001b[1;32m   1285\u001b[0m         temp_file,\n\u001b[1;32m   1286\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1287\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1288\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1289\u001b[0m     )\n\u001b[1;32m   1291\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, blob_path)\n\u001b[1;32m   1292\u001b[0m _chmod_and_replace(temp_file\u001b[39m.\u001b[39mname, blob_path)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:530\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[1;32m    520\u001b[0m     displayed_name \u001b[39m=\u001b[39m content_disposition\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39mfilename=\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    522\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    523\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    524\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[1;32m    529\u001b[0m )\n\u001b[0;32m--> 530\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[1;32m    531\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    532\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:822\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[39mraise\u001b[39;00m ContentDecodingError(e)\n\u001b[1;32m    821\u001b[0m \u001b[39mexcept\u001b[39;00m ReadTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 822\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e)\n\u001b[1;32m    823\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    824\u001b[0m     \u001b[39mraise\u001b[39;00m RequestsSSLError(e)\n","\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out."]}],"source":["# import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('bert-base-uncased')\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:50.490642Z","iopub.status.busy":"2021-05-20T12:16:50.490321Z","iopub.status.idle":"2021-05-20T12:16:50.677576Z","shell.execute_reply":"2021-05-20T12:16:50.676797Z","shell.execute_reply.started":"2021-05-20T12:16:50.490613Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<AxesSubplot:>"]},"execution_count":40,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUhklEQVR4nO3df5Dcd13H8efbxhboYdIfzE0niV7QiFMbleamrYMyd8aBNEVSFZl2OpBgnYxji8XWoUFG66jMBBURRsSJpkPQyhURprEtQgw9Gf5IpamlSVtKryVIbkIqtASPVjH69o/9nG7Pu9z+SHZv83k+Zm7uu5/vZ3df++32td/97vc2kZlIkurxXf0OIEnqLYtfkipj8UtSZSx+SaqMxS9JlVnW7wAnc+GFF+bIyEjb1/v2t7/Nueeee+oDnUaDlnnQ8oKZe2XQMg9aXlg884EDB76emS9bcEJmLtmf9evXZyfuu+++jq7XT4OWedDyZpq5VwYt86DlzVw8M/BAnqRbPdQjSZWx+CWpMha/JFXG4pekylj8klQZi1+SKmPxS1JlLH5JqozFL0mVWdJf2dArI9vvaWne4R1XneYkknT6uccvSZVZtPgj4vaIeDoiDjWN/UFEfDEiHo6IT0TEiqZ174iIqYh4PCJe2zS+sYxNRcT2U/5IJEktaWWP/0PAxjlje4FLMvNHgC8B7wCIiIuBa4AfLtf504g4KyLOAj4AXAlcDFxb5kqSemzR4s/MzwLPzBn7dGaeKBf3A6vK8mZgIjP/IzO/DEwBl5Wfqcx8KjO/A0yUuZKkHovGN3guMiliBLg7My+ZZ93fAXdm5l9FxJ8A+zPzr8q6XcAny9SNmflLZfxNwOWZeeM8t7cN2AYwPDy8fmJiou0HNTMzw9DQUMvzD04fb2neupXL287SqnYz99ug5QUz98qgZR60vLB45vHx8QOZObrQ+q7O6omIdwIngDu6uZ1mmbkT2AkwOjqaY2Njbd/G5OQk7Vxva6tn9VzXfpZWtZu53wYtL5i5VwYt86Dlhe4zd1z8EbEVeB2wIf/vbcM0sLpp2qoyxknGJUk91NHpnBGxEXg78PrMfK5p1R7gmog4JyLWAGuBfwI+D6yNiDURcTaND4D3dBddktSJRff4I+IjwBhwYUQcAW6jcRbPOcDeiIDGcf1fzsxHIuKjwKM0DgHdkJn/VW7nRuBTwFnA7Zn5yGl4PJKkRSxa/Jl57TzDu04y/13Au+YZvxe4t610kqRTzr/claTKWPySVBmLX5IqY/FLUmUsfkmqjMUvSZWx+CWpMha/JFXG4pekylj8klQZi1+SKmPxS1JlLH5JqozFL0mVsfglqTIWvyRVxuKXpMpY/JJUGYtfkipj8UtSZSx+SaqMxS9JlbH4JakyFr8kVcbil6TKLFr8EXF7RDwdEYeaxs6PiL0R8UT5fV4Zj4h4f0RMRcTDEXFp03W2lPlPRMSW0/NwJEmLaWWP/0PAxjlj24F9mbkW2FcuA1wJrC0/24APQuOFArgNuBy4DLht9sVCktRbixZ/Zn4WeGbO8GZgd1neDVzdNP7hbNgPrIiIi4DXAnsz85nMfBbYy/9/MZEk9UBk5uKTIkaAuzPzknL5m5m5oiwH8GxmroiIu4Edmfm5sm4fcCswBrwoM3+vjP8m8Hxm/uE897WNxrsFhoeH109MTLT9oGZmZhgaGmp5/sHp4y3NW7dyedtZWtVu5n4btLxg5l4ZtMyDlhcWzzw+Pn4gM0cXWr+s2wCZmRGx+KtH67e3E9gJMDo6mmNjY23fxuTkJO1cb+v2e1qad/i69rO0qt3M/TZoecHMvTJomQctL3SfudOzeo6VQziU30+X8WlgddO8VWVsoXFJUo91Wvx7gNkzc7YAdzWNv7mc3XMFcDwzjwKfAl4TEeeVD3VfU8YkST226KGeiPgIjWP0F0bEERpn5+wAPhoR1wNfAd5Ypt8LbAKmgOeAtwBk5jMR8bvA58u838nMuR8YS5J6YNHiz8xrF1i1YZ65CdywwO3cDtzeVjpJ0innX+5KUmUsfkmqjMUvSZWx+CWpMha/JFXG4pekylj8klQZi1+SKmPxS1JlLH5JqozFL0mVsfglqTIWvyRVxuKXpMpY/JJUGYtfkipj8UtSZSx+SaqMxS9JlbH4JakyFr8kVcbil6TKWPySVBmLX5Iq01XxR8SvRcQjEXEoIj4SES+KiDURcX9ETEXEnRFxdpl7Trk8VdaPnJJHIElqS8fFHxErgV8FRjPzEuAs4Brg3cB7M/MHgGeB68tVrgeeLePvLfMkST3W7aGeZcCLI2IZ8BLgKPBTwMfK+t3A1WV5c7lMWb8hIqLL+5cktSkys/MrR9wEvAt4Hvg0cBOwv+zVExGrgU9m5iURcQjYmJlHyrongcsz8+tzbnMbsA1geHh4/cTERNu5ZmZmGBoaann+wenjLc1bt3J521la1W7mfhu0vGDmXhm0zIOWFxbPPD4+fiAzRxdav6zTO46I82jsxa8Bvgn8DbCx09ublZk7gZ0Ao6OjOTY21vZtTE5O0s71tm6/p6V5h69rP0ur2s3cb4OWF8zcK4OWedDyQveZuznU89PAlzPzXzPzP4GPA68CVpRDPwCrgOmyPA2sBijrlwPf6OL+JUkd6Kb4/wW4IiJeUo7VbwAeBe4D3lDmbAHuKst7ymXK+s9kN8eZJEkd6bj4M/N+Gh/SPggcLLe1E7gVuDkipoALgF3lKruAC8r4zcD2LnJLkjrU8TF+gMy8DbhtzvBTwGXzzP134Be6ub92jbR47F6SauJf7kpSZSx+SaqMxS9JlbH4JakyFr8kVcbil6TKWPySVBmLX5IqY/FLUmUsfkmqjMUvSZWx+CWpMha/JFXG4pekylj8klQZi1+SKmPxS1JlLH5JqozFL0mVsfglqTIWvyRVxuKXpMpY/JJUGYtfkipj8UtSZboq/ohYEREfi4gvRsRjEfHjEXF+ROyNiCfK7/PK3IiI90fEVEQ8HBGXnpqHIElqR7d7/O8D/j4zfwj4UeAxYDuwLzPXAvvKZYArgbXlZxvwwS7vW5LUgY6LPyKWA68GdgFk5ncy85vAZmB3mbYbuLosbwY+nA37gRURcVGn9y9J6kxkZmdXjPgxYCfwKI29/QPATcB0Zq4ocwJ4NjNXRMTdwI7M/FxZtw+4NTMfmHO722i8I2B4eHj9xMRE29lmZmYYGhri4PTxjh7bQtatXH5Kb6/ZbOZBMWh5wcy9MmiZBy0vLJ55fHz8QGaOLrR+WRf3vQy4FHhrZt4fEe/j/w7rAJCZGRFtvbJk5k4aLyiMjo7m2NhY28EmJycZGxtj6/Z72r7uyRy+rv0srZrNPCgGLS+YuVcGLfOg5YXuM3dzjP8IcCQz7y+XP0bjheDY7CGc8vvpsn4aWN10/VVlTJLUQx0Xf2Z+DfhqRLyiDG2gcdhnD7CljG0B7irLe4A3l7N7rgCOZ+bRTu9fktSZbg71ALwVuCMizgaeAt5C48XkoxFxPfAV4I1l7r3AJmAKeK7MlST1WFfFn5kPAfN9gLBhnrkJ3NDN/UmSuudf7kpSZSx+SaqMxS9Jlen2w111YaTp7wxuWXdiwb87OLzjql5FklQB9/glqTIWvyRVxuKXpMpY/JJUGT/cbcNIi1/65oexkpYy9/glqTIWvyRVxuKXpMpY/JJUGYtfkirjWT2nQatn/0hSP7jHL0mVsfglqTIWvyRVxuKXpMpY/JJUGYtfkipj8UtSZSx+SaqMxS9Jlem6+CPirIj454i4u1xeExH3R8RURNwZEWeX8XPK5amyfqTb+5Ykte9U7PHfBDzWdPndwHsz8weAZ4Hry/j1wLNl/L1lniSpx7oq/ohYBVwF/EW5HMBPAR8rU3YDV5flzeUyZf2GMl+S1EPd7vH/MfB24L/L5QuAb2bmiXL5CLCyLK8EvgpQ1h8v8yVJPRSZ2dkVI14HbMrMX4mIMeDXga3A/nI4h4hYDXwyMy+JiEPAxsw8UtY9CVyemV+fc7vbgG0Aw8PD6ycmJtrONjMzw9DQEAenj3f02Pph+MVw7Pn5161buby3YVowu40HiZl7Y9AyD1peWDzz+Pj4gcwcXWh9N1/L/Crg9RGxCXgR8D3A+4AVEbGs7NWvAqbL/GlgNXAkIpYBy4FvzL3RzNwJ7AQYHR3NsbGxtoNNTk4yNjbG1gH6euRb1p3gPQfn/89x+Lqx3oZpwew2HiRm7o1ByzxoeaH7zB0f6snMd2TmqswcAa4BPpOZ1wH3AW8o07YAd5XlPeUyZf1nstO3G5Kkjp2O8/hvBW6OiCkax/B3lfFdwAVl/GZg+2m4b0nSIk7Jv8CVmZPAZFl+Crhsnjn/DvzCqbg/SVLn/MtdSaqMxS9JlbH4JakyFr8kVcbil6TKWPySVBmLX5IqY/FLUmUsfkmqjMUvSZWx+CWpMha/JFXG4pekylj8klQZi1+SKmPxS1JlLH5JqozFL0mVsfglqTIWvyRVxuKXpMpY/JJUGYtfkipj8UtSZSx+SapMx8UfEasj4r6IeDQiHomIm8r4+RGxNyKeKL/PK+MREe+PiKmIeDgiLj1VD0KS1LplXVz3BHBLZj4YES8FDkTEXmArsC8zd0TEdmA7cCtwJbC2/FwOfLD81iJGtt/T8tzDO646jUkknQk63uPPzKOZ+WBZ/jfgMWAlsBnYXabtBq4uy5uBD2fDfmBFRFzU6f1LkjoTmdn9jUSMAJ8FLgH+JTNXlPEAns3MFRFxN7AjMz9X1u0Dbs3MB+bc1jZgG8Dw8PD6iYmJtvPMzMwwNDTEwenjnT+oHht+MRx7vvvbWbdyefc30oLZbTxIzNwbg5Z50PLC4pnHx8cPZOboQuu7OdQDQEQMAX8LvC0zv9Xo+obMzIho65UlM3cCOwFGR0dzbGys7UyTk5OMjY2xtY1DJP12y7oTvOdg1/85OHzdWPdhWjC7jQeJmXtj0DIPWl7oPnNXZ/VExHfTKP07MvPjZfjY7CGc8vvpMj4NrG66+qoyJknqoW7O6glgF/BYZv5R06o9wJayvAW4q2n8zeXsniuA45l5tNP7lyR1pptjC68C3gQcjIiHythvADuAj0bE9cBXgDeWdfcCm4Ap4DngLV3ctySpQx0Xf/mQNhZYvWGe+Qnc0On9SZJODf9yV5IqY/FLUmUsfkmqjMUvSZWx+CWpMha/JFXG4pekylj8klQZi1+SKmPxS1JlLH5JqozFL0mVsfglqTIWvyRVxuKXpMpY/JJUGYtfkirTzT+9qCVoZPs9Lc07vOOq05xE0lLlHr8kVcbil6TKWPySVBmLX5IqY/FLUmUsfkmqjKdzVsrTPqV69bz4I2Ij8D7gLOAvMnNHrzOof3zBkfqvp4d6IuIs4APAlcDFwLURcXEvM0hS7Xq9x38ZMJWZTwFExASwGXi0xznUooX20G9Zd4KtLe69n06tvoOA1jO3+m6jnftuhe9y1CuRmb27s4g3ABsz85fK5TcBl2fmjU1ztgHbysVXAI93cFcXAl/vMm6vDVrmQcsLZu6VQcs8aHlh8czfl5kvW2jlkvtwNzN3Aju7uY2IeCAzR09RpJ4YtMyDlhfM3CuDlnnQ8kL3mXt9Ouc0sLrp8qoyJknqkV4X/+eBtRGxJiLOBq4B9vQ4gyRVraeHejLzRETcCHyKxumct2fmI6fhrro6VNQng5Z50PKCmXtl0DIPWl7o9nB4Lz/clST1n1/ZIEmVsfglqTJnVPFHxMaIeDwipiJie7/zzCciVkfEfRHxaEQ8EhE3lfHfjojpiHio/Gzqd9ZmEXE4Ig6WbA+UsfMjYm9EPFF+n9fvnLMi4hVN2/KhiPhWRLxtqW3niLg9Ip6OiENNY/Nu12h4f3l+PxwRly6RvH8QEV8smT4RESvK+EhEPN+0rf+s13lPknnB50FEvKNs48cj4rVLKPOdTXkPR8RDZbz97ZyZZ8QPjQ+LnwReDpwNfAG4uN+55sl5EXBpWX4p8CUaX1/x28Cv9zvfSXIfBi6cM/b7wPayvB14d79znuS58TXg+5badgZeDVwKHFpsuwKbgE8CAVwB3L9E8r4GWFaW392Ud6R53hLbxvM+D8r/i18AzgHWlE45aylknrP+PcBvdbqdz6Q9/v/9OojM/A4w+3UQS0pmHs3MB8vyvwGPASv7m6pjm4HdZXk3cHX/opzUBuDJzPxKv4PMlZmfBZ6ZM7zQdt0MfDgb9gMrIuKingQt5submZ/OzBPl4n4af5+zZCywjReyGZjIzP/IzC8DUzS6padOljkiAngj8JFOb/9MKv6VwFebLh9hiRdqRIwArwTuL0M3lrfLty+lwyZFAp+OiAPlazUAhjPzaFn+GjDcn2iLuoYX/k+ylLczLLxdB+E5/os03pXMWhMR/xwR/xgRP9mvUAuY73kwCNv4J4FjmflE01hb2/lMKv6BEhFDwN8Cb8vMbwEfBL4f+DHgKI23ckvJT2TmpTS+WfWGiHh188psvOdccucGlz8UfD3wN2VoqW/nF1iq23U+EfFO4ARwRxk6CnxvZr4SuBn464j4nn7lm2OgngdzXMsLd2Ta3s5nUvEPzNdBRMR30yj9OzLz4wCZeSwz/ysz/xv4c/rw9vJkMnO6/H4a+ASNfMdmDzWU30/3L+GCrgQezMxjsPS3c7HQdl2yz/GI2Aq8DriuvFhRDpd8oywfoHG8/Af7FrLJSZ4HS3YbA0TEMuDngDtnxzrZzmdS8Q/E10GU43O7gMcy84+axpuP1f4scGjudfslIs6NiJfOLtP4MO8Qje27pUzbAtzVn4Qn9YK9o6W8nZsstF33AG8uZ/dcARxvOiTUN9H4x5XeDrw+M59rGn9ZNP4NDiLi5cBa4Kn+pHyhkzwP9gDXRMQ5EbGGRuZ/6nW+k/hp4IuZeWR2oKPt3OtPq0/zJ+GbaJwl8yTwzn7nWSDjT9B46/4w8FD52QT8JXCwjO8BLup31qbML6dxpsMXgEdmty1wAbAPeAL4B+D8fmedk/tc4BvA8qaxJbWdabwoHQX+k8bx5OsX2q40zub5QHl+HwRGl0jeKRrHxWefz39W5v58eb48BDwI/MwS2sYLPg+Ad5Zt/Dhw5VLJXMY/BPzynLltb2e/skGSKnMmHeqRJLXA4pekylj8klQZi1+SKmPxS1JlLH5JqozFL0mV+R8mDJk6k0T/ZwAAAABJRU5ErkJggg==","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# get length of all the messages in the train set\n","seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins = 30)"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">4. Tokenize & Encode the Sequences</h1>"]},{"cell_type":"markdown","metadata":{},"source":["<u><h2 style=\"font-size:170%; font-family:cursive;\">Which Tokenization strategy is used by BERT?</h2></u>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">BERT uses WordPiece tokenization. The vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the existing words in the vocabulary are iteratively added.</p>\n","<br>\n","<u><h2 style=\"font-size:170%; font-family:cursive;\">What is the maximum sequence length of the input?</h2></u>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">The maximum sequence length of the input = 512</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:51.367829Z","iopub.status.busy":"2021-05-20T12:16:51.367515Z","iopub.status.idle":"2021-05-20T12:16:52.037957Z","shell.execute_reply":"2021-05-20T12:16:52.037114Z","shell.execute_reply.started":"2021-05-20T12:16:51.367800Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["# tokenize and encode sequences in the training set\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = 25,\n","    pad_to_max_length=True,\n","    truncation=True\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = 25,\n","    pad_to_max_length=True,\n","    truncation=True\n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = 25,\n","    pad_to_max_length=True,\n","    truncation=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">5. List to Tensors</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:52.707444Z","iopub.status.busy":"2021-05-20T12:16:52.707085Z","iopub.status.idle":"2021-05-20T12:16:52.744645Z","shell.execute_reply":"2021-05-20T12:16:52.743449Z","shell.execute_reply.started":"2021-05-20T12:16:52.707415Z"},"trusted":true},"outputs":[],"source":["## convert lists to tensors\n","\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">6. Data Loader</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:54.073069Z","iopub.status.busy":"2021-05-20T12:16:54.072724Z","iopub.status.idle":"2021-05-20T12:16:54.079387Z","shell.execute_reply":"2021-05-20T12:16:54.078289Z","shell.execute_reply.started":"2021-05-20T12:16:54.073019Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">7. Model Architecture</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:55.343696Z","iopub.status.busy":"2021-05-20T12:16:55.343359Z","iopub.status.idle":"2021-05-20T12:16:55.349996Z","shell.execute_reply":"2021-05-20T12:16:55.348883Z","shell.execute_reply.started":"2021-05-20T12:16:55.343669Z"},"trusted":true},"outputs":[],"source":["# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:02.994288Z","iopub.status.busy":"2021-05-20T12:23:02.993890Z","iopub.status.idle":"2021-05-20T12:23:03.006243Z","shell.execute_reply":"2021-05-20T12:23:03.005349Z","shell.execute_reply.started":"2021-05-20T12:23:02.994244Z"},"trusted":true},"outputs":[],"source":["class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(BERT_Arch, self).__init__()\n","        \n","        self.bert = bert \n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","      \n","        # relu activation function\n","        self.relu =  nn.ReLU()\n","\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768,512)\n","      \n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512,2)\n","\n","        #softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","        \n","        #pass the inputs to the model  \n","        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n","      \n","        x = self.fc1(cls_hs)\n","\n","        x = self.relu(x)\n","\n","        x = self.dropout(x)\n","\n","        # output layer\n","        x = self.fc2(x)\n","      \n","        # apply softmax activation\n","        x = self.softmax(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:03.953226Z","iopub.status.busy":"2021-05-20T12:23:03.952902Z","iopub.status.idle":"2021-05-20T12:23:03.968064Z","shell.execute_reply":"2021-05-20T12:23:03.967316Z","shell.execute_reply.started":"2021-05-20T12:23:03.953198Z"},"trusted":true},"outputs":[],"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert)\n","\n","# push the model to GPU\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:04.903784Z","iopub.status.busy":"2021-05-20T12:23:04.903449Z","iopub.status.idle":"2021-05-20T12:23:04.910844Z","shell.execute_reply":"2021-05-20T12:23:04.909852Z","shell.execute_reply.started":"2021-05-20T12:23:04.903750Z"},"trusted":true},"outputs":[],"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(),lr = 1e-5) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:05.840566Z","iopub.status.busy":"2021-05-20T12:23:05.840224Z","iopub.status.idle":"2021-05-20T12:23:05.849927Z","shell.execute_reply":"2021-05-20T12:23:05.848870Z","shell.execute_reply.started":"2021-05-20T12:23:05.840537Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Class Weights: [0.57743559 3.72848948]\n"]}],"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","#compute the class weights\n","class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n","\n","print(\"Class Weights:\",class_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:06.962480Z","iopub.status.busy":"2021-05-20T12:23:06.962150Z","iopub.status.idle":"2021-05-20T12:23:06.967108Z","shell.execute_reply":"2021-05-20T12:23:06.966225Z","shell.execute_reply.started":"2021-05-20T12:23:06.962452Z"},"trusted":true},"outputs":[],"source":["\n","# converting list of class weights to a tensor\n","weights= torch.tensor(class_weights,dtype=torch.float)\n","\n","# push to GPU\n","weights = weights.to(device)\n","\n","# define the loss function\n","cross_entropy  = nn.NLLLoss(weight=weights) \n","\n","# number of training epochs\n","epochs = 10"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">8. Fine - Tune</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:08.233269Z","iopub.status.busy":"2021-05-20T12:23:08.232922Z","iopub.status.idle":"2021-05-20T12:23:08.241591Z","shell.execute_reply":"2021-05-20T12:23:08.240484Z","shell.execute_reply.started":"2021-05-20T12:23:08.233239Z"},"trusted":true},"outputs":[],"source":["# function to train the model\n","def train():\n","    \n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","  \n","    # empty list to save model predictions\n","    total_preds=[]\n","  \n","    # iterate over batches\n","    for step,batch in enumerate(train_dataloader):\n","        \n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","        \n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n"," \n","        sent_id, mask, labels = batch\n","        \n","        # clear previously calculated gradients \n","        model.zero_grad()        \n","\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters\n","        optimizer.step()\n","\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","  \n","      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","      # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    #returns the loss and predictions\n","    return avg_loss, total_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:09.063944Z","iopub.status.busy":"2021-05-20T12:23:09.063641Z","iopub.status.idle":"2021-05-20T12:23:09.073677Z","shell.execute_reply":"2021-05-20T12:23:09.070943Z","shell.execute_reply.started":"2021-05-20T12:23:09.063916Z"},"trusted":true},"outputs":[],"source":["# function for evaluating the model\n","def evaluate():\n","    \n","    print(\"\\nEvaluating...\")\n","  \n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","    \n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step,batch in enumerate(val_dataloader):\n","        \n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            \n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","            \n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds,labels)\n","\n","            total_loss = total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","\n","            total_preds.append(preds)\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(val_dataloader) \n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:09.897241Z","iopub.status.busy":"2021-05-20T12:23:09.896772Z","iopub.status.idle":"2021-05-20T12:24:20.568390Z","shell.execute_reply":"2021-05-20T12:24:20.567287Z","shell.execute_reply.started":"2021-05-20T12:23:09.897201Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Epoch 1 / 10\n","  Batch    50  of    122.\n","  Batch   100  of    122.\n","\n","Evaluating...\n","\n","Training Loss: 0.675\n","Validation Loss: 0.650\n","\n"," Epoch 2 / 10\n","  Batch    50  of    122.\n","  Batch   100  of    122.\n","\n","Evaluating...\n","\n","Training Loss: 0.639\n","Validation Loss: 0.618\n","\n"," Epoch 3 / 10\n","  Batch    50  of    122.\n","  Batch   100  of    122.\n","\n","Evaluating...\n","\n","Training Loss: 0.615\n","Validation Loss: 0.590\n","\n"," Epoch 4 / 10\n","  Batch    50  of    122.\n","  Batch   100  of    122.\n","\n","Evaluating...\n","\n","Training Loss: 0.588\n","Validation Loss: 0.566\n","\n"," Epoch 5 / 10\n","  Batch    50  of    122.\n","  Batch   100  of    122.\n","\n","Evaluating...\n","\n","Training Loss: 0.567\n","Validation Loss: 0.541\n","\n"," Epoch 6 / 10\n","  Batch    50  of    122.\n","  Batch   100  of    122.\n","\n","Evaluating...\n","\n","Training Loss: 0.545\n","Validation Loss: 0.520\n","\n"," Epoch 7 / 10\n","  Batch    50  of    122.\n","  Batch   100  of    122.\n","\n","Evaluating...\n","\n","Training Loss: 0.528\n","Validation Loss: 0.499\n","\n"," Epoch 8 / 10\n","  Batch    50  of    122.\n","  Batch   100  of    122.\n","\n","Evaluating...\n","\n","Training Loss: 0.506\n","Validation Loss: 0.478\n","\n"," Epoch 9 / 10\n","  Batch    50  of    122.\n","  Batch   100  of    122.\n","\n","Evaluating...\n","\n","Training Loss: 0.493\n","Validation Loss: 0.460\n","\n"," Epoch 10 / 10\n","  Batch    50  of    122.\n","  Batch   100  of    122.\n","\n","Evaluating...\n","\n","Training Loss: 0.475\n","Validation Loss: 0.444\n"]}],"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss, _ = train()\n","    \n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    \n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:24:47.832686Z","iopub.status.busy":"2021-05-20T12:24:47.832372Z","iopub.status.idle":"2021-05-20T12:24:48.147778Z","shell.execute_reply":"2021-05-20T12:24:48.147016Z","shell.execute_reply.started":"2021-05-20T12:24:47.832657Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["#load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">9. Make Predictions</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:25:07.436008Z","iopub.status.busy":"2021-05-20T12:25:07.435685Z","iopub.status.idle":"2021-05-20T12:25:08.019816Z","shell.execute_reply":"2021-05-20T12:25:08.018976Z","shell.execute_reply.started":"2021-05-20T12:25:07.435979Z"},"trusted":true},"outputs":[],"source":["# get predictions for test data\n","with torch.no_grad():\n","    preds = model(test_seq.to(device), test_mask.to(device))\n","    preds = preds.detach().cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:25:23.838719Z","iopub.status.busy":"2021-05-20T12:25:23.838389Z","iopub.status.idle":"2021-05-20T12:25:23.852193Z","shell.execute_reply":"2021-05-20T12:25:23.850795Z","shell.execute_reply.started":"2021-05-20T12:25:23.838691Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.97      0.88      0.92       724\n","           1       0.51      0.81      0.63       112\n","\n","    accuracy                           0.87       836\n","   macro avg       0.74      0.85      0.77       836\n","weighted avg       0.91      0.87      0.88       836\n","\n"]}],"source":["# model's performance\n","preds = np.argmax(preds, axis = 1)\n","print(classification_report(test_y, preds))"]},{"cell_type":"markdown","metadata":{},"source":["<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">REFERENCES & CREDITS</h1></center>"]},{"cell_type":"markdown","metadata":{},"source":["<ol>\n","    <li style=\"font-size:150%;\"><a href=\"https://www.reddit.com/r/MachineLearning/comments/ao23cp/p_how_to_use_bert_in_kaggle_competitions_a/\">How to use BERT in Kaggle competitions - Reddit Thread</a></li>\n","    <li style=\"font-size:150%;\"><a href=\"http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\">A visual guide to using BERT by Jay Alammar</a></li>\n","    <li style=\"font-size:150%;\"><a href=\"https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/\">Demystifying BERT: Groundbreaking NLP Framework by Mohd Sanad Zaki Rizvi</a></li>\n","    <li style=\"font-size:150%;\"><a href=\"https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\">BERT for Dummies step by step tutorial by Michel Kana</a></li>\n","</ol>"]},{"cell_type":"markdown","metadata":{},"source":["<br>\n","<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">CONCLUSION</h1></center>"]},{"cell_type":"markdown","metadata":{},"source":["<p style=\"font-size:150%; font-family:verdana;\">BERT is undoubtedly a breakthrough in the use of Machine Learning for Natural Language Processing. The fact that it’s approachable and allows fast fine-tuning will likely allow a wide range of practical applications in the future. In this Notebook we have discussed about BERT (Theoritical + Practical Part).</p>"]},{"cell_type":"markdown","metadata":{},"source":["<center><h1 style=\"font-size:200%; color:green;\">Please give this kernel an UPVOTE to show your appreciation, if you find it useful.</h1></center>"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
