{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Details\n",
    "\n",
    "## Submitted By\n",
    "Desh Iyer, 500081889, Year III, AI/ML(H), B5\n",
    "\n",
    "## Problem Statement\n",
    "For a given set of training data examples stored in a `.csv` file, implement and demonstrate the candidate elimination algorithm to output a description of the set of all hypotheses consistent with the training examples.\n",
    "\n",
    "## Theory\n",
    "The Candidate-Elimination algorithm is a concept learning algorithm used to find the set of all hypotheses that are consistent with a given set of training data samples. The algorithm maintains two sets of hypotheses: the set of most specific hypotheses S and the set of most general hypotheses G. Initially, S contains the most specific hypothesis possible, and G contains the most general hypothesis possible. The algorithm iteratively updates S and G based on the training data.\n",
    "\n",
    "Here are the terms used in the description of the algorithm.\n",
    "\n",
    "- A hypothesis h is a conjunction of n literals, where each literal can take one of two values: true or false.\n",
    "- A training example is a pair (x, y), where x is an n-dimensional vector of attribute values, and y is either true or false.\n",
    "- A positive example is a training example where y is true.\n",
    "- A negative example is a training example where y is false.\n",
    "\n",
    "## Advantages\n",
    "- The algorithm can handle noisy data and errors in the training data.\n",
    "- The algorithm outputs a set of hypotheses that are consistent with the training data, which can be used for further analysis or decision making.\n",
    "\n",
    "## Limitations\n",
    "- The algorithm can become computationally expensive when the number of attributes or the size of the hypothesis space is large.\n",
    "- The algorithm does not provide any measure of the quality or confidence of the hypotheses.\n",
    "- The algorithm assumes that the target concept is represented by a single consistent hypothesis.\n",
    "\n",
    "## Pseudocode\n",
    "The Candidate-Elimination algorithm starts by initializing S with the most specific hypothesis possible, and G with the most general hypothesis possible:\n",
    "\n",
    "```shell\n",
    "S ← { < ⊥, ⊥, ..., ⊥ > }\n",
    "G ← { < ?, ?, ..., ? > }\n",
    "```\n",
    "\n",
    "Then, for each training example (x, y) in the training data, the algorithm updates S and G based on whether the example is positive or negative. For positive examples, the algorithm updates S to include only the hypotheses that are consistent with the example, and updates G to remove any hypotheses that are inconsistent with the example. For negative examples, the algorithm updates G to include only the hypotheses that are consistent with the example, and updates S to remove any hypotheses that are inconsistent with the example.\n",
    "\n",
    "```shell\n",
    "for each training example (x, y) do\n",
    "    if y = true then\n",
    "        S ← { h belongs to S : h(x) = y } # Keep only the hypotheses that are consistent with x.\n",
    "        for g belongs to G do\n",
    "        if g(x) != y and g is still in G then\n",
    "            G ← G - { g } # Remove any hypotheses that are inconsistent with x.\n",
    "            # add to G all minimal generalizations of h \n",
    "            # that are consistent with all positive training examples seen so far\n",
    "        end if\n",
    "        end for\n",
    "    else # y = false\n",
    "        G ← { h ∈ G : h(x) != y } # Keep only the hypotheses that are consistent with x.\n",
    "        for s belongs to S do\n",
    "        if s(x) = y and s is still in S then\n",
    "            S ← S - { s } # Remove any hypotheses that are inconsistent with x.\n",
    "            # add to S all minimal specializations of h \n",
    "            # that are consistent with all negative training examples seen so far\n",
    "        end if\n",
    "        end for\n",
    "    end if\n",
    "end for\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instances are:\n",
      " [['Sunny' 'Warm' 'High' 'Strong' 'Warm' 'Same']\n",
      " ['Rainy' 'Cold' 'High' 'Strong' 'Warm' 'Change']\n",
      " ['Sunny' 'Warm' 'High' 'Strong' 'Cool' 'Change']]\n",
      "\n",
      "Target values are:\n",
      " ['Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/candidate-elimination.csv')\n",
    "\n",
    "concepts = np.array(data.iloc[:, 0:-1])\n",
    "target = np.array(data.iloc[:, -1])\n",
    "\n",
    "print(\"\\nInstances are:\\n\",concepts)\n",
    "print(\"\\nTarget values are:\\n\",target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Function to Learn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(concepts, target):\n",
    "    specific_h = concepts[0]\n",
    "    general_h = [[\"?\" for i in range(len(specific_h))] for i in range(len(specific_h))]\n",
    "\n",
    "    print(\"\\nInitializing hypotheses\")\n",
    "    print(\"\\nSpecific Boundary: \", specific_h)\n",
    "    print(\"\\nGeneric Boundary:\\n\",general_h)\n",
    "\n",
    "    for i, h in enumerate(concepts):\n",
    "        print(\"\\nInstance\", i + 1 , \"is \", h)\n",
    "        if target[i] == \"yes\":\n",
    "            print(\"Instance is Positive\")\n",
    "            for x in range(len(specific_h)):\n",
    "                if h[x] != specific_h[x]:\n",
    "                    specific_h[x] ='?'\n",
    "                    general_h[x][x] ='?'\n",
    "\n",
    "        if target[i] == \"no\":\n",
    "            print(\"Instance is Negative \")\n",
    "            for x in range(len(specific_h)):\n",
    "                if h[x] != specific_h[x]:\n",
    "                    general_h[x][x] = specific_h[x]\n",
    "                else:\n",
    "                    general_h[x][x] = '?'\n",
    "\n",
    "        print(\"Specific boundary after\", i + 1, \"iteration is \", specific_h)\n",
    "        print(\"Generic boundary after\", i + 1, \"iteration is \", general_h)\n",
    "\n",
    "    indices = [i for i, val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']]\n",
    "    for i in indices:\n",
    "        general_h.remove(['?', '?', '?', '?', '?', '?'])\n",
    "    return specific_h, general_h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing hypotheses\n",
      "\n",
      "Specific Boundary:  ['Sunny' 'Warm' 'High' 'Strong' 'Warm' 'Same']\n",
      "\n",
      "Generic Boundary:\n",
      " [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
      "\n",
      "Instance 1 is  ['Sunny' 'Warm' 'High' 'Strong' 'Warm' 'Same']\n",
      "Specific boundary after 1 iteration is  ['Sunny' 'Warm' 'High' 'Strong' 'Warm' 'Same']\n",
      "Generic boundary after 1 iteration is  [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
      "\n",
      "Instance 2 is  ['Rainy' 'Cold' 'High' 'Strong' 'Warm' 'Change']\n",
      "Specific boundary after 2 iteration is  ['Sunny' 'Warm' 'High' 'Strong' 'Warm' 'Same']\n",
      "Generic boundary after 2 iteration is  [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
      "\n",
      "Instance 3 is  ['Sunny' 'Warm' 'High' 'Strong' 'Cool' 'Change']\n",
      "Specific boundary after 3 iteration is  ['Sunny' 'Warm' 'High' 'Strong' 'Warm' 'Same']\n",
      "Generic boundary after 3 iteration is  [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
      "\n",
      "Final Specific Hypothesis: \n",
      "['Sunny' 'Warm' 'High' 'Strong' 'Warm' 'Same']\n",
      "Final General Hypothesis: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "s_final, g_final = learn(concepts, target)\n",
    "\n",
    "print(\"\\nFinal Specific Hypothesis: \", s_final, sep=\"\\n\")\n",
    "print(\"Final General Hypothesis: \", g_final, sep=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
