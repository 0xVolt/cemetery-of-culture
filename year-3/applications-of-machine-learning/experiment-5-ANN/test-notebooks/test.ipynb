{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network from Scratch\n",
    "Our NN will have a simple two-layer architecture. Input layer $a^{[0]}$ will have 784 units corresponding to the 784 pixels in each 28x28 input image. A hidden layer $a^{[1]}$ will have 10 units with ReLU activation, and finally our output layer $a^{[2]}$ will have 10 units corresponding to the ten digit classes with softmax activation.\n",
    "\n",
    "**Forward propagation**\n",
    "\n",
    "$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n",
    "$$A^{[1]} = g_{\\text{ReLU}}(Z^{[1]}))$$\n",
    "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n",
    "$$A^{[2]} = g_{\\text{softmax}}(Z^{[2]})$$\n",
    "\n",
    "**Backward propagation**\n",
    "\n",
    "$$dZ^{[2]} = A^{[2]} - Y$$\n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
    "$$dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$$\n",
    "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})$$\n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}$$\n",
    "$$dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$$\n",
    "\n",
    "**Parameter updates**\n",
    "\n",
    "$$W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n",
    "$$b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n",
    "$$W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n",
    "$$b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$\n",
    "\n",
    "**Vars and shapes**\n",
    "\n",
    "Forward prop\n",
    "\n",
    "- $A^{[0]} = X$: 784 x m\n",
    "- $Z^{[1]} \\sim A^{[1]}$: 10 x m\n",
    "- $W^{[1]}$: 10 x 784 (as $W^{[1]} A^{[0]} \\sim Z^{[1]}$)\n",
    "- $B^{[1]}$: 10 x 1\n",
    "- $Z^{[2]} \\sim A^{[2]}$: 10 x m\n",
    "- $W^{[1]}$: 10 x 10 (as $W^{[2]} A^{[1]} \\sim Z^{[2]}$)\n",
    "- $B^{[2]}$: 10 x 1\n",
    "\n",
    "Backprop\n",
    "\n",
    "- $dZ^{[2]}$: 10 x m ($~A^{[2]}$)\n",
    "- $dW^{[2]}$: 10 x 10\n",
    "- $dB^{[2]}$: 10 x 1\n",
    "- $dZ^{[1]}$: 10 x m ($~A^{[1]}$)\n",
    "- $dW^{[1]}$: 10 x 10\n",
    "- $dB^{[1]}$: 10 x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = r'digit-recognizer\\train.csv'\n",
    "\n",
    "data = pd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded the data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the data from a dataframe to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that the model isn't overfitted, i.e., the model makes fairly accurate predictions for the training data but isn't generalised for the data it's supposed to have a high accuracy for. Setting aside a portion of the training data to perform cross-validation on to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the data before we split the data into dev and training data. Note, `np.random.shuffle()` permutes the sequence in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 0, 0, ..., 0, 0, 0],\n",
       "       [6, 0, 0, ..., 0, 0, 0],\n",
       "       [4, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [6, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [3, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing the dimensions\n",
    "m, n = data.shape\n",
    "\n",
    "# m - Number of images; n - label + pixels for each image\n",
    "m, n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into dev and training. We're using dev to cross validate and we're setting aside only 1000 images to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposing the data using only 1000 images\n",
    "data_dev = data[:1000].T\n",
    "\n",
    "# Storing the labels in YDev\n",
    "Y_dev = data_dev[0]\n",
    "\n",
    "# Storing the pixels\n",
    "X_dev = data_dev[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the rest of the images\n",
    "data_train = data[1000:m].T\n",
    "\n",
    "# Extract labels\n",
    "Y_train = data_train[0]\n",
    "\n",
    "# Get the rest\n",
    "X_train = data_train[1:]\n",
    "X_train = X_train / 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing details of all arrays implemented so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing dimensions of all existing arrays:\n",
      "(i) X - pixels\n",
      "X_dev: (784, 1000)\n",
      "X_train: (784, 41000)\n",
      "\n",
      "(ii) Y - labels\n",
      "Y_dev: (1000,)\n",
      "Y_train: (41000,)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Printing dimensions of all existing arrays:\\n(i) X - pixels\\nX_dev: {X_dev.shape}\\nX_train: {X_train.shape}\\n\\n(ii) Y - labels\\nY_dev: {Y_dev.shape}\\nY_train: {Y_train.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining Initial Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to initialise the neural network by creating random weights. We use `rand()` to obtain a random value between 0 and 1 and then we subtract from those values to make sure the range in which our random values lie is `[-0.5, 0.5]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    # # There's 10 connections for each of the 784 nodes\n",
    "    # W1 = np.random.rand(10, n - 1) - 0.5\n",
    "\n",
    "    # # There's 10 biases\n",
    "    # b1 = np.random.rand(10, 1) - 0.5\n",
    "\n",
    "    # # Similarly,\n",
    "    # # There's 10 connections to 10 output nodes\n",
    "    # W2 = np.random.rand(10, 10) - 0.5\n",
    "    # b2 = np.random.rand(10, 1) - 0.5\n",
    "\n",
    "    # return W1, b1, W2, b2\n",
    "\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function implementing the ReLU (rectified linear unit) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    # Taking the maximum element-wise using numpy\n",
    "    return np.maximum(0, Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function implementing the softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    \n",
    "    # Returning the probability\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to implement forward propagation through the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(W1, b1, W2, b2, X):\n",
    "    # # Deactivated first layer\n",
    "    # Z1 = W1.dot(X) + b1\n",
    "    \n",
    "    # # Activating Z1\n",
    "    # A1 = ReLU(Z1)\n",
    "    \n",
    "    # # Creating the next layer's deactivated input\n",
    "    # Z2 = W2.dot(A1) + b2\n",
    "    \n",
    "    # # Since the next layer is the output layer, we apply softmax\n",
    "    # A2 = softmax(Z2)\n",
    "    \n",
    "    # return Z1, A1, Z2, A2\n",
    "\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to implement one-hot encoding of Y. This is to represent the target classes as an array instead of a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(Y):\n",
    "    # Encoding\n",
    "    one_hot_encoded_df = pd.get_dummies(Y)\n",
    "\n",
    "    # Taking the transpose so the columns represent images\n",
    "    one_hot_encoded_array = np.array(one_hot_encoded_df).T\n",
    "\n",
    "    return one_hot_encoded_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to illustrate the working of `one_hot_encode(Y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 0, 5, 8, 7, 1, 6, 7, 5, 6, 4, 8, 3, 6, 2, 2, 2, 8, 7, 6],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Y_train[:20]\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.get_dummies(test).T\n",
    "df\n",
    "\n",
    "ls = np.array(df)\n",
    "ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing it to our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encode(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative one-hot encoding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    \n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    \n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    \n",
    "    return one_hot_Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to implement the derivative of ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_ReLU(Z):\n",
    "    # Returning 1 if the value is greater than 0. This is because the slope of the `linear` thing is 1.\n",
    "    return Z > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to back propagate through the neural network to calculate the differences in the weights and biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def back_propagation(Z1, A1, Z2, A2, W2, X, Y):\n",
    "    m = Y.size\n",
    "    # one_hot_encoded_Y = one_hot_encode(Y)\n",
    "\n",
    "    # dZ2 = A2 - one_hot_encoded_Y\n",
    "    \n",
    "    # one_hot_Y = one_hot(Y)\n",
    "    # dZ2 = A2 - one_hot_Y\n",
    "    \n",
    "    # dW2 = (1 / m) * dZ2.dot(A1.T)\n",
    "    # db2 = (1 / m) * np.sum(dZ2)\n",
    "\n",
    "    # dZ1 = W2.T.dot(dZ2) * derivative_ReLU(Z1)\n",
    "    # dW1 = (1 / m) * dZ1.dot(X.T)\n",
    "    # db1 = (1 / m) * np.sum(dZ1)\n",
    "    \n",
    "    # return dW1, db1, dW2, db2\n",
    "\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Updating Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to update the parameters using learning rate `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    # W1 = W1 - alpha * dW1\n",
    "    # b1 = b1 - alpha * db1\n",
    "    \n",
    "    # W2 = W2 - alpha * dW2\n",
    "    # b2 = b2 - alpha * db2\n",
    "    \n",
    "    # return W1, b1, W2, b2\n",
    "\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Defining Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A):\n",
    "    # Returns the indices of the max values\n",
    "    return np.argmax(A, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the `get_predictions()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8, 0, 5, 8, 7, 1, 6, 7, 5, 6, 4, 8, 3, 6, 2, 2, 2, 8, 7, 6],\n",
       "       dtype=int64),\n",
       " 0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test, get_predictions(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    \n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks.  Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error. Once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (AI) and computer science applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, epochs, alpha):\n",
    "    # Defining weights and biases\n",
    "    # W1, b1, W2, b2 = init_params()\n",
    "\n",
    "    # for i in range(epochs):\n",
    "    #     # Step 1\n",
    "    #     Z1, A1, Z2, A2 = forward_propagation(W1, b1, W2, b2, X)\n",
    "        \n",
    "    #     # Step 2\n",
    "    #     dW1, db1, dW2, db2 = back_propagation(Z1, A1, Z2, A2, W2, X, Y)\n",
    "        \n",
    "    #     # Step 3\n",
    "    #     W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        \n",
    "    #     # Every 10th iteration\n",
    "    #     if i % 10 == 0:\n",
    "    #         print(\"Iteration: \", i)\n",
    "            \n",
    "    #         # A2 is the output from the forward propagation\n",
    "    #         predictions = get_predictions(A2)\n",
    "            \n",
    "    #         print(get_accuracy(predictions, Y))\n",
    "    \n",
    "    # return W1, b1, W2, b2\n",
    "\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(epochs):\n",
    "        Z1, A1, Z2, A2 = forward_propagation(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = back_propagation(Z1, A1, Z2, A2, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Running the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[2 7 7 ... 0 0 0] [8 0 5 ... 6 0 3]\n",
      "0.10078048780487806\n",
      "Iteration:  10\n",
      "[2 7 9 ... 4 0 0] [8 0 5 ... 6 0 3]\n",
      "0.20131707317073172\n",
      "Iteration:  20\n",
      "[2 7 9 ... 4 0 1] [8 0 5 ... 6 0 3]\n",
      "0.28573170731707315\n",
      "Iteration:  30\n",
      "[2 0 7 ... 4 0 1] [8 0 5 ... 6 0 3]\n",
      "0.3539024390243902\n",
      "Iteration:  40\n",
      "[8 0 7 ... 6 0 1] [8 0 5 ... 6 0 3]\n",
      "0.40929268292682924\n",
      "Iteration:  50\n",
      "[8 0 5 ... 6 0 1] [8 0 5 ... 6 0 3]\n",
      "0.4617317073170732\n",
      "Iteration:  60\n",
      "[8 0 5 ... 6 0 1] [8 0 5 ... 6 0 3]\n",
      "0.5089756097560976\n",
      "Iteration:  70\n",
      "[8 0 5 ... 6 0 8] [8 0 5 ... 6 0 3]\n",
      "0.5497317073170732\n",
      "Iteration:  80\n",
      "[8 0 5 ... 6 0 8] [8 0 5 ... 6 0 3]\n",
      "0.5806097560975609\n",
      "Iteration:  90\n",
      "[8 0 5 ... 6 0 8] [8 0 5 ... 6 0 3]\n",
      "0.6091951219512195\n",
      "Iteration:  100\n",
      "[8 0 5 ... 6 0 8] [8 0 5 ... 6 0 3]\n",
      "0.634\n",
      "Iteration:  110\n",
      "[8 0 5 ... 6 0 8] [8 0 5 ... 6 0 3]\n",
      "0.6561463414634147\n",
      "Iteration:  120\n",
      "[8 0 5 ... 6 0 8] [8 0 5 ... 6 0 3]\n",
      "0.6760975609756098\n",
      "Iteration:  130\n",
      "[8 0 5 ... 6 0 8] [8 0 5 ... 6 0 3]\n",
      "0.6951463414634146\n",
      "Iteration:  140\n",
      "[8 0 5 ... 6 0 8] [8 0 5 ... 6 0 3]\n",
      "0.7107560975609756\n",
      "Iteration:  150\n",
      "[8 0 5 ... 6 0 8] [8 0 5 ... 6 0 3]\n",
      "0.7242682926829268\n",
      "Iteration:  160\n",
      "[8 0 5 ... 6 0 8] [8 0 5 ... 6 0 3]\n",
      "0.7360487804878049\n",
      "Iteration:  170\n",
      "[8 0 5 ... 6 0 8] [8 0 5 ... 6 0 3]\n",
      "0.7465853658536585\n",
      "Iteration:  180\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.7563658536585366\n",
      "Iteration:  190\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.7648536585365854\n",
      "Iteration:  200\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.7715853658536586\n",
      "Iteration:  210\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.7788780487804878\n",
      "Iteration:  220\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.7854146341463415\n",
      "Iteration:  230\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.7908048780487805\n",
      "Iteration:  240\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.7955121951219513\n",
      "Iteration:  250\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8007804878048781\n",
      "Iteration:  260\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8051951219512196\n",
      "Iteration:  270\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8097317073170732\n",
      "Iteration:  280\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8134878048780487\n",
      "Iteration:  290\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8170243902439024\n",
      "Iteration:  300\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8204390243902439\n",
      "Iteration:  310\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8232682926829268\n",
      "Iteration:  320\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8262195121951219\n",
      "Iteration:  330\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8290975609756097\n",
      "Iteration:  340\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8312195121951219\n",
      "Iteration:  350\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8335609756097561\n",
      "Iteration:  360\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.835390243902439\n",
      "Iteration:  370\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8377073170731707\n",
      "Iteration:  380\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8400731707317073\n",
      "Iteration:  390\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8415121951219512\n",
      "Iteration:  400\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.843390243902439\n",
      "Iteration:  410\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8446585365853658\n",
      "Iteration:  420\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8462926829268292\n",
      "Iteration:  430\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.847609756097561\n",
      "Iteration:  440\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8490243902439024\n",
      "Iteration:  450\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8502926829268292\n",
      "Iteration:  460\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8520243902439024\n",
      "Iteration:  470\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8531219512195122\n",
      "Iteration:  480\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.8542682926829268\n",
      "Iteration:  490\n",
      "[8 0 5 ... 6 0 3] [8 0 5 ... 6 0 3]\n",
      "0.855390243902439\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 500, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a00e68b57149134ce2b73879182c8cdd95184339958c7ea15c51bb9d3595f11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
