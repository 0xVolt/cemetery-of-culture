{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0xVolt/cemetery-of-culture/blob/main/year-3/neural-networks/6-sentiment-analysis/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTs2MRi7LMK9"
      },
      "outputs": [],
      "source": [
        "#impoting the required lib\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing\n",
        "import keras\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opIiYpwa55v3"
      },
      "source": [
        "only keeping the required coloumns "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYY_SbAs5-QV"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('Sentiment.csv')\n",
        "data = data[['text','sentiment']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl2W1m-XTtGJ",
        "outputId": "0e289eb5-f7f9-42e0-b1da-7b4c919d7c68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4472\n",
            "16986\n"
          ]
        }
      ],
      "source": [
        "data = data[data.sentiment != \"Neutral\"]\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "\n",
        "print(data[ data['sentiment'] == 'Positive'].size)\n",
        "print(data[ data['sentiment'] == 'Negative'].size)\n",
        "\n",
        "for idx,row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt',' ')\n",
        "    \n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "X = pad_sequences(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM4Cs6lfUd9J"
      },
      "source": [
        "the variables embed_dim, lstm_out, batch_size, droupout_x are hyperparameters (the parameters used to control the learning process By contrast the values of the other parameters are derived via traning)thier values are intuitive and can be played with to achive better results "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwhNoFbjWdhw"
      },
      "source": [
        "softmax activation funtion is used as the network uses catagorical crossentropy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl2Z48MXWzLT",
        "outputId": "fc7085b1-569f-4767-ac60-4a20e84fb549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 28, 128)           256000    \n",
            "                                                                 \n",
            " spatial_dropout1d_2 (Spatia  (None, 28, 128)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 196)               254800    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 394       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 511,194\n",
            "Trainable params: 511,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "embed_dim=128\n",
        "lstm_out=196\n",
        "model = Sequential()#each layer has one input tensor and one output tensor \n",
        "model.add(Embedding(max_fatures, embed_dim, input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))#dropsout 1D feature maps insted of individual elements\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QybHnSgad4T7"
      },
      "source": [
        "traning and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIx-QwU0d8Th",
        "outputId": "9edf4777-c135-428e-8e7f-3e38697e3d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7188, 28) (7188, 2)\n",
            "(3541, 28) (3541, 2)\n"
          ]
        }
      ],
      "source": [
        "Y = pd.get_dummies(data['sentiment']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b2Ljdj0idP7"
      },
      "source": [
        "traning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2siBzIGJh_3X",
        "outputId": "a2e92402-77d3-4ae4-8595-15235c0342c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "225/225 - 33s - loss: 0.4309 - accuracy: 0.8166 - 33s/epoch - 149ms/step\n",
            "Epoch 2/10\n",
            "225/225 - 31s - loss: 0.3178 - accuracy: 0.8666 - 31s/epoch - 138ms/step\n",
            "Epoch 3/10\n",
            "225/225 - 30s - loss: 0.2834 - accuracy: 0.8822 - 30s/epoch - 133ms/step\n",
            "Epoch 4/10\n",
            "225/225 - 31s - loss: 0.2486 - accuracy: 0.8986 - 31s/epoch - 136ms/step\n",
            "Epoch 5/10\n",
            "225/225 - 30s - loss: 0.2233 - accuracy: 0.9080 - 30s/epoch - 131ms/step\n",
            "Epoch 6/10\n",
            "225/225 - 31s - loss: 0.2028 - accuracy: 0.9175 - 31s/epoch - 137ms/step\n",
            "Epoch 7/10\n",
            "225/225 - 30s - loss: 0.1828 - accuracy: 0.9242 - 30s/epoch - 132ms/step\n",
            "Epoch 8/10\n",
            "225/225 - 31s - loss: 0.1652 - accuracy: 0.9342 - 31s/epoch - 136ms/step\n",
            "Epoch 9/10\n",
            "225/225 - 29s - loss: 0.1475 - accuracy: 0.9399 - 29s/epoch - 131ms/step\n",
            "Epoch 10/10\n",
            "225/225 - 31s - loss: 0.1372 - accuracy: 0.9453 - 31s/epoch - 136ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbd2fdfba90>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 32\n",
        "model.fit(X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfrCpfFBiXBN"
      },
      "source": [
        "extracting a validation set and mesuring the score and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hjcCtqliiGQF",
        "outputId": "49c00213-1079-476d-b4f2-aefb3707b153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64/64 - 2s - loss: 0.5063 - accuracy: 0.8236 - 2s/epoch - 24ms/step\n",
            "score: 0.51\n",
            "acc: 0.82\n"
          ]
        }
      ],
      "source": [
        "validation_size = 1500\n",
        "\n",
        "X_validate = X_test[-validation_size:]\n",
        "Y_validate = Y_test[-validation_size:]\n",
        "X_test = X_test[:-validation_size]\n",
        "Y_test = Y_test[:-validation_size]\n",
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXw_ilTSiO3J"
      },
      "source": [
        "mesuring the numner of correct gueses "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkcdkJgmiNVd"
      },
      "outputs": [],
      "source": [
        "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "for x in range(len(X_validate)):\n",
        "    \n",
        "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
        "   \n",
        "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
        "        if np.argmax(Y_validate[x]) == 0:\n",
        "            neg_correct += 1\n",
        "        else:\n",
        "            pos_correct += 1\n",
        "       \n",
        "    if np.argmax(Y_validate[x]) == 0:\n",
        "        neg_cnt += 1\n",
        "    else:\n",
        "        pos_cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
        "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}